#!/usr/bin/env python
"""
Test script to analyze chinese_90s.wav using FixedVADIterator with chunk processing
This script processes a long WAV file in chunks and applies VAD to each chunk,
ensuring consistent VAD effects across the entire audio.
"""

import os
import sys
import time
import numpy as np
import soundfile as sf
import torch
import logging
import matplotlib.pyplot as plt
from tqdm import tqdm

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add the whisper_streaming directory to the path
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'whisper_streaming'))
from silero_vad_iterator import FixedVADIterator

def load_silero_vad():
    """
    Load the Silero VAD model
    
    Returns:
        model: Silero VAD model
    """
    model, _ = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad'
    )
    return model

def read_audio(file_path, sampling_rate=16000):
    """
    Read audio file and convert to the right format
    
    Args:
        file_path: Path to the audio file
        sampling_rate: Target sampling rate
        
    Returns:
        audio_data: Audio data as float32 numpy array
    """
    # Load audio file
    audio_data, file_sample_rate = sf.read(file_path)
    
    # Ensure audio is mono
    if len(audio_data.shape) > 1:
        audio_data = audio_data[:, 0]
    
    # Ensure audio is float32
    if audio_data.dtype != np.float32:
        audio_data = audio_data.astype(np.float32)
    
    # Resample to target sampling rate if needed
    if file_sample_rate != sampling_rate:
        import librosa
        audio_data = librosa.resample(audio_data, orig_sr=file_sample_rate, target_sr=sampling_rate)
    
    return audio_data

def get_speech_timestamps(audio_data, model, threshold=0.5, sampling_rate=16000, 
                         min_silence_duration_ms=500, speech_pad_ms=100, return_seconds=False):
    """
    Get speech timestamps from audio data using Silero VAD
    
    Args:
        audio_data: Audio data as float32 numpy array
        model: Silero VAD model
        threshold: Speech threshold (0.0-1.0)
        sampling_rate: Audio sampling rate
        min_silence_duration_ms: Minimum silence duration in ms
        speech_pad_ms: Padding in ms to add to each side of a speech segment
        return_seconds: Whether to return timestamps in seconds (default is samples)
        
    Returns:
        speech_timestamps: List of dicts with 'start' and 'end' keys
    """
    # Convert to tensor
    audio_tensor = torch.tensor(audio_data)
    
    # Get speech timestamps
    speech_timestamps = []
    
    # Process in chunks of 512 samples
    chunk_size = 512
    window_size_samples = chunk_size
    
    # Initialize variables
    triggered = False
    speeches = []
    current_speech = {}
    neg_threshold = threshold - 0.15
    temp_end = 0
    
    # Convert to samples
    min_silence_samples = sampling_rate * min_silence_duration_ms / 1000
    speech_pad_samples = sampling_rate * speech_pad_ms / 1000
    
    for i in range(0, len(audio_data), chunk_size):
        chunk = audio_data[i:i+chunk_size]
        
        # Pad with zeros if needed
        if len(chunk) < chunk_size:
            chunk = np.pad(chunk, (0, chunk_size - len(chunk)), 'constant')
        
        # Get speech probability
        speech_prob = model(torch.tensor([chunk]), sampling_rate).item()
        
        # Update triggered state
        if (speech_prob >= threshold) and temp_end:
            temp_end = 0
            
        if (speech_prob >= threshold) and not triggered:
            triggered = True
            current_speech['start'] = max(0, i - speech_pad_samples)
            continue
            
        if (speech_prob < neg_threshold) and triggered:
            if not temp_end:
                temp_end = i + chunk_size
            
            if i + chunk_size - temp_end < min_silence_samples:
                continue
            
            current_speech['end'] = temp_end + speech_pad_samples
            speeches.append(current_speech)
            current_speech = {}
            temp_end = 0
            triggered = False
    
    # Add the last speech segment if triggered
    if triggered and 'start' in current_speech:
        current_speech['end'] = len(audio_data)
        speeches.append(current_speech)
    
    # Convert to seconds if requested
    if return_seconds:
        for speech in speeches:
            speech['start'] = speech['start'] / sampling_rate
            speech['end'] = speech['end'] / sampling_rate
    
    return speeches

def test_batch_vad(audio_file):
    """ä½¿ç”¨batchæ–¹å¼å¤„ç†æ•´ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œè·å–è¯­éŸ³æ®µæ—¶é—´æˆ³"""
    model = load_silero_vad()
    wav = read_audio(audio_file)
    
    # ä½¿ç”¨æŒ‡å®šå‚æ•°
    threshold = 0.3
    min_silence_duration_ms = 300
    speech_pad_ms = 100
    
    speech_timestamps = get_speech_timestamps(
        wav,
        model,
        threshold=threshold,
        min_silence_duration_ms=min_silence_duration_ms,
        speech_pad_ms=speech_pad_ms,
        return_seconds=True,  # Return speech timestamps in seconds (default is samples)
    )
    print("\n===== Batch VAD å¤„ç†ç»“æœ =====")
    print(f"å‚æ•°: threshold={threshold}, min_silence_duration_ms={min_silence_duration_ms}, speech_pad_ms={speech_pad_ms}")
    print(f"æ£€æµ‹åˆ° {len(speech_timestamps)} ä¸ªè¯­éŸ³æ®µ")
    return speech_timestamps

def load_audio_file(file_path, sample_rate=16000):
    """
    Load audio file and convert to the right format
    """
    logger.info(f"Loading audio file: {file_path}")
    
    # Load audio file
    audio_data, file_sample_rate = sf.read(file_path)
    
    # Print audio properties
    print(f"\n===== Audio File Properties =====")
    print(f"File: {file_path}")
    print(f"Sample rate: {file_sample_rate} Hz")
    print(f"Channels: {1 if len(audio_data.shape) == 1 else audio_data.shape[1]}")
    print(f"Duration: {len(audio_data)/file_sample_rate:.2f} seconds")
    print(f"Data type: {audio_data.dtype}")
    print(f"Min value: {np.min(audio_data)}")
    print(f"Max value: {np.max(audio_data)}")
    print(f"Mean value: {np.mean(audio_data)}")
    print(f"RMS value: {np.sqrt(np.mean(np.square(audio_data)))}")
    
    # Ensure audio is mono
    if len(audio_data.shape) > 1:
        print(f"Converting stereo to mono")
        audio_data = audio_data[:, 0]
    
    # Ensure audio is float32
    if audio_data.dtype != np.float32:
        print(f"Converting {audio_data.dtype} to float32")
        audio_data = audio_data.astype(np.float32)
    
    # Resample to 16kHz if needed
    if file_sample_rate != sample_rate:
        import librosa
        print(f"Resampling from {file_sample_rate}Hz to {sample_rate}Hz")
        audio_data = librosa.resample(audio_data, orig_sr=file_sample_rate, target_sr=sample_rate)
    
    # Plot waveform
    plt.figure(figsize=(15, 4))
    plt.plot(np.arange(len(audio_data)) / sample_rate, audio_data)
    plt.title('Audio Waveform')
    plt.xlabel('Time (s)')
    plt.ylabel('Amplitude')
    plt.tight_layout()
    plt.savefig('audio_waveform_90s.png')
    print(f"Waveform saved to audio_waveform_90s.png")
    
    logger.info(f"Audio length: {len(audio_data)/sample_rate:.2f} seconds")
    return audio_data, sample_rate

def analyze_with_fixed_vad_chunks(audio_data, sample_rate=16000, chunk_duration=10.0, 
                                 threshold=0.5, min_silence_duration_ms=500, speech_pad_ms=100):
    """
    Analyze audio data with FixedVADIterator using chunk processing
    
    Args:
        audio_data: Audio data as numpy array
        sample_rate: Sample rate of the audio
        chunk_duration: Duration of each chunk in seconds
        threshold: Speech threshold (0.0-1.0)
        min_silence_duration_ms: Minimum silence duration in ms
        speech_pad_ms: Padding in ms to add to each side of a speech segment
    """
    # Load the Silero VAD model
    model, _ = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad'
    )
    
    # Calculate chunk size in samples
    chunk_size_samples = int(chunk_duration * sample_rate)
    
    # Create the VAD iterator
    vad = FixedVADIterator(
        model=model,
        threshold=threshold,
        sampling_rate=sample_rate,
        min_silence_duration_ms=min_silence_duration_ms,
        speech_pad_ms=speech_pad_ms
    )
    
    # Process the audio in chunks
    processing_chunk_size = 512  # Exact size required by Silero VAD
    results = []
    
    print(f"\n===== Processing audio with FixedVADIterator in chunks =====")
    print(f"Parameters: threshold={threshold}, min_silence_duration_ms={min_silence_duration_ms}, speech_pad_ms={speech_pad_ms}")
    print(f"Audio length: {len(audio_data)/sample_rate:.2f} seconds")
    print(f"Processing in chunks of {chunk_duration} seconds ({chunk_size_samples} samples)")
    print(f"VAD processing chunk size: {processing_chunk_size} samples ({processing_chunk_size/sample_rate:.2f} seconds)")
    
    # Process in larger chunks (e.g., 10 seconds each)
    num_chunks = int(np.ceil(len(audio_data) / chunk_size_samples))
    
    for chunk_idx in tqdm(range(num_chunks), desc="Processing chunks"):
        # Extract the current chunk
        start_idx = chunk_idx * chunk_size_samples
        end_idx = min(start_idx + chunk_size_samples, len(audio_data))
        current_chunk = audio_data[start_idx:end_idx]
        
        # Reset VAD state for each new chunk
        vad.reset_states()
        
        # Process this chunk with VAD in smaller processing chunks
        chunk_results = []
        current_time_offset = start_idx / sample_rate
        
        # Process the current chunk in smaller processing chunks
        for i in range(0, len(current_chunk), processing_chunk_size):
            proc_chunk = current_chunk[i:i+processing_chunk_size]
            
            # Pad with zeros if needed
            if len(proc_chunk) < processing_chunk_size:
                proc_chunk = np.pad(proc_chunk, (0, processing_chunk_size - len(proc_chunk)), 'constant')
            
            # Process the chunk with VAD iterator
            result = vad(proc_chunk, return_seconds=True)
            
            if result:
                # Adjust the timestamps based on the current position
                if 'start' in result:
                    result['start'] += current_time_offset + (i / sample_rate)
                if 'end' in result:
                    result['end'] += current_time_offset + (i / sample_rate)
                
                chunk_results.append(result)
        
        # Force an end if we have a start but no end
        has_start = any('start' in r for r in chunk_results)
        has_end = any('end' in r for r in chunk_results)
        
        if has_start and not has_end:
            chunk_results.append({
                'end': current_time_offset + (len(current_chunk) / sample_rate)
            })
        
        # Add chunk results to overall results
        results.extend(chunk_results)
    
    # Post-process results to merge segments that span across chunks
    merged_results = []
    current_segment = None
    
    for result in results:
        if 'start' in result:
            if current_segment is None:
                current_segment = {'start': result['start']}
            else:
                # If we already have a start but no end, keep the earlier start
                pass
        elif 'end' in result:
            if current_segment is not None:
                # Ensure end time is after start time
                if result['end'] > current_segment['start']:
                    current_segment['end'] = result['end']
                    merged_results.append(current_segment)
                else:
                    # Skip invalid segments where end is before start
                    print(f"Warning: Skipping invalid segment with end ({result['end']}) before start ({current_segment['start']})")
                current_segment = None
    
    # Handle any remaining segment without an end
    if current_segment is not None:
        current_segment['end'] = len(audio_data) / sample_rate
        merged_results.append(current_segment)
    
    return merged_results

def analyze_with_fixed_vad_continuous(audio_data, sample_rate=16000, 
                                     threshold=0.5, min_silence_duration_ms=500, speech_pad_ms=100):
    """
    Analyze audio data with FixedVADIterator in a continuous manner
    
    Args:
        audio_data: Audio data as numpy array
        sample_rate: Sample rate of the audio
        threshold: Speech threshold (0.0-1.0)
        min_silence_duration_ms: Minimum silence duration in ms
        speech_pad_ms: Padding in ms to add to each side of a speech segment
    """
    # Load the Silero VAD model
    model, _ = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad'
    )
    
    # Create the VAD iterator
    vad = FixedVADIterator(
        model=model,
        threshold=threshold,
        sampling_rate=sample_rate,
        min_silence_duration_ms=min_silence_duration_ms,
        speech_pad_ms=speech_pad_ms
    )
    
    # Process the audio in chunks
    processing_chunk_size = 512  # Exact size required by Silero VAD
    results = []
    last_chunk_index = (len(audio_data) - 1) // processing_chunk_size
    
    print(f"\n===== Processing audio with FixedVADIterator continuously =====")
    print(f"Parameters: threshold={threshold}, min_silence_duration_ms={min_silence_duration_ms}, speech_pad_ms={speech_pad_ms}")
    print(f"Audio length: {len(audio_data)/sample_rate:.2f} seconds")
    print(f"VAD processing chunk size: {processing_chunk_size} samples ({processing_chunk_size/sample_rate:.2f} seconds)")
    
    # Process the entire audio continuously
    for i in tqdm(range(0, len(audio_data), processing_chunk_size), desc="Processing audio"):
        chunk = audio_data[i:i+processing_chunk_size]
        is_last_chunk = (i // processing_chunk_size) == last_chunk_index
        
        # Pad with zeros if needed
        if len(chunk) < processing_chunk_size:
            chunk = np.pad(chunk, (0, processing_chunk_size - len(chunk)), 'constant')
        
        # Process the chunk with VAD iterator
        result = vad(chunk, return_seconds=True)
        
        if result:
            results.append(result)
        
        # If this is the last chunk and VAD is still triggered, force an end
        if is_last_chunk and vad.triggered:
            results.append({'end': len(audio_data) / sample_rate})
            print(f"Forced end at {len(audio_data) / sample_rate:.2f}s because audio ended while speech was active")
    
    return results

def compare_vad_methods(batch_results, chunked_results, continuous_results, streaming_results=None):
    """
    Compare different VAD processing methods
    
    Args:
        batch_results: Results from batch VAD processing
        chunked_results: Results from chunked VAD processing (ä»…ç”¨äºè®¡ç®—é‡å ç‡ï¼Œä¸åœ¨å›¾ä¸­æ˜¾ç¤º)
        continuous_results: Results from continuous VAD processing
        streaming_results: Results from streaming VAD processing (optional)
    """
    print("\n===== VAD å¤„ç†æ–¹æ³•æ¯”è¾ƒ =====")
    
    # Convert continuous results to segments
    continuous_segments = []
    start_time = None
    
    for result in continuous_results:
        if 'start' in result:
            start_time = result['start']
        elif 'end' in result and start_time is not None:
            continuous_segments.append({
                'start': start_time,
                'end': result['end'],
                'duration': result['end'] - start_time
            })
            start_time = None
    
    # Print comparison
    print(f"Batch VAD æ£€æµ‹åˆ°çš„è¯­éŸ³æ®µæ•°é‡: {len(batch_results)}")
    print(f"Continuous VAD æ£€æµ‹åˆ°çš„è¯­éŸ³æ®µæ•°é‡: {len(continuous_segments)}")
    if streaming_results:
        print(f"Streaming VAD æ£€æµ‹åˆ°çš„è¯­éŸ³æ®µæ•°é‡: {len(streaming_results)}")
    
    # Print batch segments
    print("\nBatch VAD è¯­éŸ³æ®µ:")
    print(f"{'#':<5} {'Start (s)':<15} {'End (s)':<15} {'Duration (s)':<15}")
    print("-" * 50)
    
    for i, segment in enumerate(batch_results):
        duration = segment['end'] - segment['start']
        print(f"{i+1:<5} {segment['start']:<15.2f} {segment['end']:<15.2f} {duration:<15.2f}")
    
    # Print continuous segments
    print("\nContinuous VAD è¯­éŸ³æ®µ:")
    print(f"{'#':<5} {'Start (s)':<15} {'End (s)':<15} {'Duration (s)':<15}")
    print("-" * 50)
    
    for i, segment in enumerate(continuous_segments):
        print(f"{i+1:<5} {segment['start']:<15.2f} {segment['end']:<15.2f} {segment['duration']:<15.2f}")
    
    # Print streaming segments if available
    if streaming_results:
        print("\nStreaming VAD è¯­éŸ³æ®µ:")
        print(f"{'#':<5} {'Start (s)':<15} {'End (s)':<15} {'Duration (s)':<15}")
        print("-" * 50)
        
        for i, segment in enumerate(streaming_results):
            duration = segment['end'] - segment['start']
            print(f"{i+1:<5} {segment['start']:<15.2f} {segment['end']:<15.2f} {duration:<15.2f}")
    
    # Calculate overlap between batch and continuous
    batch_continuous_overlap = calculate_overlap(batch_results, continuous_segments)
    
    print(f"\nBatch vs Continuous é‡å æ¯”ä¾‹: {batch_continuous_overlap:.2%}")
    
    # Calculate overlap with streaming results if available
    if streaming_results:
        batch_streaming_overlap = calculate_overlap(batch_results, streaming_results)
        continuous_streaming_overlap = calculate_overlap(continuous_segments, streaming_results)
        
        print(f"Batch vs Streaming é‡å æ¯”ä¾‹: {batch_streaming_overlap:.2%}")
        print(f"Continuous vs Streaming é‡å æ¯”ä¾‹: {continuous_streaming_overlap:.2%}")
    
    # Compare total duration
    batch_duration = sum(seg['end'] - seg['start'] for seg in batch_results)
    continuous_duration = sum(seg['duration'] for seg in continuous_segments)
    
    print(f"\nBatch VAD æ€»è¯­éŸ³æ—¶é•¿: {batch_duration:.2f} ç§’")
    print(f"Continuous VAD æ€»è¯­éŸ³æ—¶é•¿: {continuous_duration:.2f} ç§’")
    
    if streaming_results:
        streaming_duration = sum(seg['end'] - seg['start'] for seg in streaming_results)
        print(f"Streaming VAD æ€»è¯­éŸ³æ—¶é•¿: {streaming_duration:.2f} ç§’")
    
    # Plot the segments for visual comparison
    plot_segments_comparison(batch_results, chunked_results, continuous_segments, streaming_results)
    
    return continuous_segments
    
    for i, segment in enumerate(chunked_results):
        duration = segment['end'] - segment['start']
        print(f"{i+1:<5} {segment['start']:<15.2f} {segment['end']:<15.2f} {duration:<15.2f}")
    
    # Print continuous segments
    print("\nContinuous VAD è¯­éŸ³æ®µ:")
    print(f"{'#':<5} {'Start (s)':<15} {'End (s)':<15} {'Duration (s)':<15}")
    print("-" * 50)
    
    for i, segment in enumerate(continuous_segments):
        print(f"{i+1:<5} {segment['start']:<15.2f} {segment['end']:<15.2f} {segment['duration']:<15.2f}")
    
    # Print streaming segments if available
    if streaming_results:
        print("\nStreaming VAD è¯­éŸ³æ®µ:")
        print(f"{'#':<5} {'Start (s)':<15} {'End (s)':<15} {'Duration (s)':<15}")
        print("-" * 50)
        
        for i, segment in enumerate(streaming_results):
            duration = segment['end'] - segment['start']
            print(f"{i+1:<5} {segment['start']:<15.2f} {segment['end']:<15.2f} {duration:<15.2f}")
    
    # Calculate overlap between batch and chunked
    batch_chunked_overlap = calculate_overlap(batch_results, chunked_results)
    
    # Calculate overlap between batch and continuous
    batch_continuous_overlap = calculate_overlap(batch_results, continuous_segments)
    
    # Calculate overlap between chunked and continuous
    chunked_continuous_overlap = calculate_overlap(chunked_results, continuous_segments)
    
    print(f"\nBatch vs Chunked é‡å æ¯”ä¾‹: {batch_chunked_overlap:.2%}")
    print(f"Batch vs Continuous é‡å æ¯”ä¾‹: {batch_continuous_overlap:.2%}")
    print(f"Chunked vs Continuous é‡å æ¯”ä¾‹: {chunked_continuous_overlap:.2%}")
    
    # Calculate overlap with streaming results if available
    if streaming_results:
        batch_streaming_overlap = calculate_overlap(batch_results, streaming_results)
        chunked_streaming_overlap = calculate_overlap(chunked_results, streaming_results)
        continuous_streaming_overlap = calculate_overlap(continuous_segments, streaming_results)
        
        print(f"Batch vs Streaming é‡å æ¯”ä¾‹: {batch_streaming_overlap:.2%}")
        print(f"Chunked vs Streaming é‡å æ¯”ä¾‹: {chunked_streaming_overlap:.2%}")
        print(f"Continuous vs Streaming é‡å æ¯”ä¾‹: {continuous_streaming_overlap:.2%}")
    
    # Compare total duration
    batch_duration = sum(seg['end'] - seg['start'] for seg in batch_results)
    chunked_duration = sum(seg['end'] - seg['start'] for seg in chunked_results)
    continuous_duration = sum(seg['duration'] for seg in continuous_segments)
    
    print(f"\nBatch VAD æ€»è¯­éŸ³æ—¶é•¿: {batch_duration:.2f} ç§’")
    print(f"Chunked VAD æ€»è¯­éŸ³æ—¶é•¿: {chunked_duration:.2f} ç§’")
    print(f"Continuous VAD æ€»è¯­éŸ³æ—¶é•¿: {continuous_duration:.2f} ç§’")
    
    if streaming_results:
        streaming_duration = sum(seg['end'] - seg['start'] for seg in streaming_results)
        print(f"Streaming VAD æ€»è¯­éŸ³æ—¶é•¿: {streaming_duration:.2f} ç§’")
    
    # Plot the segments for visual comparison
    plot_segments_comparison(batch_results, chunked_results, continuous_segments, streaming_results)
    
    return continuous_segments

def calculate_overlap(segments1, segments2):
    """
    Calculate overlap ratio between two sets of segments
    """
    overlap_count = 0
    
    for seg1 in segments1:
        seg1_start = seg1['start'] if 'start' in seg1 else seg1['start']
        seg1_end = seg1['end'] if 'end' in seg1 else seg1['end']
        
        for seg2 in segments2:
            seg2_start = seg2['start'] if 'start' in seg2 else seg2['start']
            seg2_end = seg2['end'] if 'end' in seg2 else seg2['end']
            
            # Check if there is overlap
            if (seg2_start <= seg1_end and seg2_end >= seg1_start):
                overlap_count += 1
                break
    
    overlap_ratio = overlap_count / len(segments1) if segments1 else 0
    return overlap_ratio

def plot_segments_comparison(batch_segments, chunked_segments, continuous_segments, streaming_segments=None):
    """
    Plot segments from different VAD methods for visual comparison
    
    Args:
        batch_segments: Segments from batch VAD processing
        chunked_segments: Segments from chunked VAD processing (ä¸åœ¨å›¾ä¸­æ˜¾ç¤º)
        continuous_segments: Segments from continuous VAD processing
        streaming_segments: Segments from streaming VAD processing (optional)
    """
    plt.figure(figsize=(20, 10 if streaming_segments else 8))
    
    # Plot audio waveform as background
    try:
        audio_data, sample_rate = sf.read("chinese_180s.wav")
        if len(audio_data.shape) > 1:
            audio_data = audio_data[:, 0]  # Convert to mono
        
        # Normalize and scale the waveform
        audio_data = audio_data / np.max(np.abs(audio_data)) * 0.3
        
        # Calculate audio duration
        audio_duration = len(audio_data) / sample_rate
        
        # Plot waveform with transparency
        time_axis = np.arange(len(audio_data)) / sample_rate
        plt.plot(time_axis, audio_data + (2 if streaming_segments else 1.5), color='gray', alpha=0.3, linewidth=0.5)
        
        print(f"éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f} ç§’")
    except Exception as e:
        print(f"æ— æ³•ç»˜åˆ¶æ³¢å½¢: {e}")
        audio_duration = 180.0  # å¦‚æœæ— æ³•è¯»å–éŸ³é¢‘æ–‡ä»¶ï¼Œé»˜è®¤ä¸º180ç§’
    
    # è®¡ç®—æ¯ç§æ–¹æ³•çš„yä½ç½® (ç§»é™¤chunked)
    y_positions = {}
    if streaming_segments:
        y_positions = {'batch': 3, 'continuous': 2, 'streaming': 1}
    else:
        y_positions = {'batch': 2, 'continuous': 1}
    
    # ä¸ºæ¯ç§æ–¹æ³•åˆ›å»ºå‚ç›´åç§»å­—å…¸ï¼Œç”¨äºé¿å…é‡å 
    vertical_offsets = {'batch': {}, 'continuous': {}, 'streaming': {}}
    
    # æ‰¾å‡ºæ‰€æœ‰æ®µçš„æœ€å¤§ç»“æŸæ—¶é—´ï¼Œç”¨äºè®¾ç½®xè½´èŒƒå›´
    max_end_time = 0
    
    # åˆ›å»ºæ›´å°çš„æ—¶é—´åŒºé—´ä»¥æ›´å¥½åœ°å¤„ç†é‡å 
    bin_size = 3  # 3ç§’çš„åŒºé—´
    
    # è®¡ç®—æ‰¹å¤„ç†æ®µçš„å‚ç›´åç§»
    for segment in batch_segments:
        start_bin = int(segment['start'] / bin_size)
        if start_bin not in vertical_offsets['batch']:
            vertical_offsets['batch'][start_bin] = 0
        else:
            vertical_offsets['batch'][start_bin] += 0.15  # å¢åŠ å‚ç›´åç§»
    
    # ç»˜åˆ¶æ‰¹å¤„ç†æ®µ
    for i, segment in enumerate(batch_segments):
        start_bin = int(segment['start'] / bin_size)
        offset = vertical_offsets['batch'][start_bin]
        y_pos = y_positions['batch'] + offset
        
        plt.plot([segment['start'], segment['end']], [y_pos, y_pos], 'b-', linewidth=3)
        
        # åªä¸ºè¾ƒé•¿çš„æ®µæ·»åŠ æ—¶é•¿æ ‡ç­¾ï¼Œé¿å…æ‹¥æŒ¤
        duration = segment['end'] - segment['start']
        if duration > 1.0:  # åªä¸ºè¶…è¿‡1ç§’çš„æ®µæ·»åŠ æ ‡ç­¾
            plt.text((segment['start'] + segment['end']) / 2, y_pos + 0.1, 
                    f"{duration:.1f}s", 
                    ha='center', fontsize=7)
        
        max_end_time = max(max_end_time, segment['end'])
        
        # å‡å°‘è¯¥åŒºé—´çš„åç§»é‡ï¼Œä¸ºä¸‹ä¸€ä¸ªæ®µåšå‡†å¤‡
        vertical_offsets['batch'][start_bin] -= 0.15
    
    # è®¡ç®—è¿ç»­å¤„ç†æ®µçš„å‚ç›´åç§»
    for segment in continuous_segments:
        if 'start' in segment and 'end' in segment:
            start_bin = int(segment['start'] / bin_size)
            if start_bin not in vertical_offsets['continuous']:
                vertical_offsets['continuous'][start_bin] = 0
            else:
                vertical_offsets['continuous'][start_bin] += 0.15
    
    # ç»˜åˆ¶è¿ç»­å¤„ç†æ®µ
    for i, segment in enumerate(continuous_segments):
        if 'start' in segment and 'end' in segment:
            start_bin = int(segment['start'] / bin_size)
            offset = vertical_offsets['continuous'][start_bin]
            y_pos = y_positions['continuous'] + offset
            
            plt.plot([segment['start'], segment['end']], [y_pos, y_pos], 'g-', linewidth=3)
            
            # åªä¸ºè¾ƒé•¿çš„æ®µæ·»åŠ æ—¶é•¿æ ‡ç­¾
            duration = segment['end'] - segment['start']
            if duration > 1.0:
                plt.text((segment['start'] + segment['end']) / 2, y_pos + 0.1, 
                        f"{duration:.1f}s", 
                        ha='center', fontsize=7)
            
            max_end_time = max(max_end_time, segment['end'])
            
            # å‡å°‘è¯¥åŒºé—´çš„åç§»é‡
            vertical_offsets['continuous'][start_bin] -= 0.15
    
    # å¦‚æœæœ‰æµå¼å¤„ç†æ®µï¼Œåˆ™ç»˜åˆ¶
    if streaming_segments:
        # æ£€æŸ¥æµå¼æ®µçš„æ ¼å¼
        print(f"æµå¼æ®µæ•°é‡: {len(streaming_segments)}")
        for i, segment in enumerate(streaming_segments[:5]):  # æ‰“å°å‰5ä¸ªæ®µçš„ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•
            print(f"æµå¼æ®µ {i+1}: {segment}")
        
        # è®¡ç®—æµå¼å¤„ç†æ®µçš„å‚ç›´åç§»
        for segment in streaming_segments:
            if 'start' in segment and 'end' in segment:
                start_bin = int(segment['start'] / bin_size)
                if start_bin not in vertical_offsets['streaming']:
                    vertical_offsets['streaming'][start_bin] = 0
                else:
                    vertical_offsets['streaming'][start_bin] += 0.15
        
        for i, segment in enumerate(streaming_segments):
            # ç¡®ä¿æµå¼æ®µæœ‰æ­£ç¡®çš„å¼€å§‹å’Œç»“æŸæ—¶é—´
            if 'start' in segment and 'end' in segment:
                # ç¡®ä¿æ—¶é—´æˆ³æ˜¯æœ‰æ•ˆçš„æ•°å€¼
                start = float(segment['start'])
                end = float(segment['end'])
                
                start_bin = int(start / bin_size)
                offset = vertical_offsets['streaming'][start_bin]
                y_pos = y_positions['streaming'] + offset
                
                # ç»˜åˆ¶æ®µ
                plt.plot([start, end], [y_pos, y_pos], 'y-', linewidth=3)
                
                # æ·»åŠ æ—¶é•¿æ ‡ç­¾
                duration = end - start
                plt.text((start + end) / 2, y_pos + 0.1, 
                        f"{duration:.1f}s", 
                        ha='center', fontsize=7)
                
                # æ›´æ–°æœ€å¤§ç»“æŸæ—¶é—´
                max_end_time = max(max_end_time, end)
                
                # å‡å°‘è¯¥åŒºé—´çš„åç§»é‡
                vertical_offsets['streaming'][start_bin] -= 0.15
    
    # ç¡®ä¿max_end_timeä¸ä¸ºé›¶ï¼Œå¹¶æ·»åŠ ä¸€äº›å¡«å……
    max_end_time = max(max_end_time, audio_duration)
    max_end_time = max_end_time * 1.05  # æ·»åŠ 5%çš„å¡«å……
    
    # æ¯10ç§’æ·»åŠ ä¸€æ¡å‚ç›´ç½‘æ ¼çº¿
    grid_interval = 10  # ç§’
    for i in range(0, int(max_end_time) + grid_interval, grid_interval):
        plt.axvline(x=i, color='gray', linestyle='--', alpha=0.3)
        plt.text(i, 0.5, f"{i}s", ha='center', fontsize=8)
    
    # æ ¹æ®å¯ç”¨æ–¹æ³•è®¾ç½®yåˆ»åº¦ (ç§»é™¤chunked)
    if streaming_segments:
        plt.yticks([1, 2, 3], ['Streaming', 'Continuous', 'Batch'])
    else:
        plt.yticks([1, 2], ['Continuous', 'Batch'])
    
    # æ·»åŠ å›¾ä¾‹
    plt.plot([], [], 'b-', linewidth=3, label='Batch VAD')
    plt.plot([], [], 'g-', linewidth=3, label='Continuous VAD')
    if streaming_segments:
        plt.plot([], [], 'y-', linewidth=3, label='Streaming VAD')
    plt.legend(loc='upper right')
    
    plt.xlabel('Time (seconds)')
    plt.title('VAD Segment Comparison')
    plt.grid(True, axis='x', alpha=0.3)
    plt.xlim(0, max_end_time)  # æ ¹æ®å®é™…æ•°æ®è®¾ç½®xè½´èŒƒå›´
    plt.ylim(0.5, 4.0 if streaming_segments else 3.0)  # è®¾ç½®yè½´èŒƒå›´ï¼Œå¢åŠ ç©ºé—´ä»¥å®¹çº³å‚ç›´åç§»
    plt.tight_layout()
    plt.savefig('vad_comparison.png', dpi=150)
    print(f"VADæ¯”è¾ƒå›¾å·²ä¿å­˜åˆ°vad_comparison.png")

def test_different_chunk_sizes(audio_data, sample_rate=16000, batch_results=None):
    """
    Test different chunk sizes for VAD processing
    """
    print("\n===== æµ‹è¯•ä¸åŒçš„åˆ†å—å¤§å° =====")
    
    # Define chunk sizes to test (in seconds)
    chunk_sizes = [3]
    
    best_chunk_size = None
    best_overlap = 0
    best_results = None
    
    # Fixed VAD parameters - using the specified values
    threshold = 0.2  # æŒ‡å®šå‚æ•°
    min_silence_duration_ms = 300  # æŒ‡å®šå‚æ•°
    speech_pad_ms = 100  # æŒ‡å®šå‚æ•°
    
    for chunk_size in chunk_sizes:
        print(f"\nåˆ†å—å¤§å°: {chunk_size} ç§’")
        chunked_results = analyze_with_fixed_vad_chunks(
            audio_data, 
            sample_rate=sample_rate,
            chunk_duration=chunk_size,
            threshold=threshold,
            min_silence_duration_ms=min_silence_duration_ms,
            speech_pad_ms=speech_pad_ms
        )
        
        # Calculate overlap with batch results
        if batch_results:
            overlap_ratio = calculate_overlap(batch_results, chunked_results)
            print(f"ä¸æ‰¹å¤„ç†çš„é‡å æ¯”ä¾‹: {overlap_ratio:.2%}")
            
            # Update best chunk size if this one has better overlap
            if overlap_ratio > best_overlap:
                best_overlap = overlap_ratio
                best_chunk_size = chunk_size
                best_results = chunked_results
    
    # Print best chunk size
    if best_chunk_size:
        print(f"\n===== æœ€ä½³åˆ†å—å¤§å° =====")
        print(f"åˆ†å—å¤§å°: {best_chunk_size} ç§’")
        print(f"é‡å æ¯”ä¾‹: {best_overlap:.2%}")
    
    return best_chunk_size, best_results

def main():
    """Main function"""
    # Audio file path
    audio_file = "chinese_180s.wav"
    
    # Load audio file
    audio_data, sample_rate = load_audio_file(audio_file)
    
    # Fixed VAD parameters - using the specified values
    threshold = 0.3  # æŒ‡å®šå‚æ•°
    min_silence_duration_ms = 300  # æŒ‡å®šå‚æ•°
    speech_pad_ms = 100  # æŒ‡å®šå‚æ•°
    
    print(f"\n===== ä½¿ç”¨å›ºå®šå‚æ•° =====")
    print(f"threshold={threshold}, min_silence_duration_ms={min_silence_duration_ms}, speech_pad_ms={speech_pad_ms}")
    
    # Run batch VAD with specified parameters
    batch_results = test_batch_vad(audio_file)
    
    # Test different chunk sizes with specified parameters
    best_chunk_size, best_chunked_results = test_different_chunk_sizes(audio_data, sample_rate, batch_results)
    
    # Process with continuous VAD using specified parameters
    continuous_results = analyze_with_fixed_vad_continuous(
        audio_data, 
        sample_rate=sample_rate,
        threshold=threshold,
        min_silence_duration_ms=min_silence_duration_ms,
        speech_pad_ms=speech_pad_ms
    )
    
    print("\n===== test_streaming_vad =====")
    streaming_results = test_streaming_vad(audio_file, 0.128)
    
    # Compare VAD methods and get processed continuous segments
    continuous_segments = compare_vad_methods(batch_results, best_chunked_results, continuous_results, streaming_results)
    
    # TODO æŠŠè¿™è¾¹çš„èŠ‚ç‚¹ å¯¹æ¯” batch_results/continuous_results/streaming_results
    print("\n" + "="*80)
    print("ğŸ“Š è¯¦ç»†VADæ–¹æ³•å¯¹æ¯”åˆ†æ")
    print("="*80)
    
    detailed_vad_comparison(batch_results, continuous_segments, streaming_results, len(audio_data)/sample_rate)


def detailed_vad_comparison(batch_results, continuous_results, streaming_results, audio_duration):
    """
    è¯¦ç»†å¯¹æ¯”ä¸‰ç§VADæ–¹æ³•çš„å…³é”®æŒ‡æ ‡
    
    Args:
        batch_results: æ‰¹å¤„ç†VADç»“æœ
        continuous_results: è¿ç»­VADç»“æœ  
        streaming_results: æµå¼VADç»“æœ
        audio_duration: éŸ³é¢‘æ€»æ—¶é•¿
    """
    
    # 1. åŸºç¡€ç»Ÿè®¡å¯¹æ¯”
    print("\nğŸ”¢ åŸºç¡€ç»Ÿè®¡å¯¹æ¯”")
    print("-" * 60)
    
    methods = {
        'Batch': batch_results,
        'Continuous': continuous_results, 
        'Streaming': streaming_results
    }
    
    stats = {}
    for method_name, results in methods.items():
        if results:
            # è®¡ç®—æ€»è¯­éŸ³æ—¶é•¿
            total_speech_duration = sum(seg['end'] - seg['start'] for seg in results)
            
            # è®¡ç®—å¹³å‡è¯­éŸ³æ®µé•¿åº¦
            avg_segment_duration = total_speech_duration / len(results)
            
            # è®¡ç®—æœ€é•¿å’Œæœ€çŸ­è¯­éŸ³æ®µ
            durations = [seg['end'] - seg['start'] for seg in results]
            max_duration = max(durations)
            min_duration = min(durations)
            
            # è®¡ç®—è¯­éŸ³å æ¯”
            speech_ratio = total_speech_duration / audio_duration * 100
            
            stats[method_name] = {
                'segment_count': len(results),
                'total_speech_duration': total_speech_duration,
                'avg_segment_duration': avg_segment_duration,
                'max_segment_duration': max_duration,
                'min_segment_duration': min_duration,
                'speech_ratio': speech_ratio
            }
            
            print(f"{method_name:12} | æ®µæ•°: {len(results):2d} | æ€»æ—¶é•¿: {total_speech_duration:6.1f}s | "
                  f"å¹³å‡: {avg_segment_duration:4.1f}s | æœ€é•¿: {max_duration:5.1f}s | "
                  f"æœ€çŸ­: {min_duration:4.1f}s | å æ¯”: {speech_ratio:4.1f}%")
    
    # 2. æ—¶é—´ç²¾åº¦å¯¹æ¯”
    print(f"\nâ±ï¸  æ—¶é—´ç²¾åº¦å¯¹æ¯”")
    print("-" * 60)
    
    if streaming_results and batch_results:
        # è®¡ç®—æ—¶é—´æˆ³å·®å¼‚
        timestamp_diffs = []
        for i, (batch_seg, stream_seg) in enumerate(zip(batch_results, streaming_results)):
            start_diff = abs(batch_seg['start'] - stream_seg['start'])
            end_diff = abs(batch_seg['end'] - stream_seg['end'])
            timestamp_diffs.extend([start_diff, end_diff])
            
            if i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªæ®µçš„è¯¦ç»†å¯¹æ¯”
                print(f"æ®µ{i+1:2d} | Batch: {batch_seg['start']:6.2f}-{batch_seg['end']:6.2f}s | "
                      f"Stream: {stream_seg['start']:6.2f}-{stream_seg['end']:6.2f}s | "
                      f"å·®å¼‚: {start_diff:4.2f}s/{end_diff:4.2f}s")
        
        avg_timestamp_diff = sum(timestamp_diffs) / len(timestamp_diffs)
        max_timestamp_diff = max(timestamp_diffs)
        print(f"\næ—¶é—´æˆ³å·®å¼‚ç»Ÿè®¡: å¹³å‡ {avg_timestamp_diff:.3f}s, æœ€å¤§ {max_timestamp_diff:.3f}s")
    
    # 3. è¯­éŸ³æ®µåˆ†å¸ƒåˆ†æ
    print(f"\nğŸ“ˆ è¯­éŸ³æ®µæ—¶é•¿åˆ†å¸ƒåˆ†æ")
    print("-" * 60)
    
    duration_ranges = [(0, 1), (1, 3), (3, 5), (5, 10), (10, float('inf'))]
    range_labels = ['<1s', '1-3s', '3-5s', '5-10s', '>10s']
    
    for method_name, results in methods.items():
        if results:
            durations = [seg['end'] - seg['start'] for seg in results]
            distribution = []
            
            for min_dur, max_dur in duration_ranges:
                count = sum(1 for d in durations if min_dur <= d < max_dur)
                percentage = count / len(durations) * 100
                distribution.append(f"{count:2d}({percentage:4.1f}%)")
            
            print(f"{method_name:12} | " + " | ".join(f"{label:>5}: {dist}" 
                  for label, dist in zip(range_labels, distribution)))
    
    # 4. ä¸€è‡´æ€§åˆ†æ
    print(f"\nğŸ¯ æ–¹æ³•ä¸€è‡´æ€§åˆ†æ")
    print("-" * 60)
    
    if len(methods) >= 2:
        method_pairs = [
            ('Batch', 'Continuous'),
            ('Batch', 'Streaming'), 
            ('Continuous', 'Streaming')
        ]
        
        for method1, method2 in method_pairs:
            if method1 in stats and method2 in stats:
                # æ®µæ•°å·®å¼‚
                count_diff = abs(stats[method1]['segment_count'] - stats[method2]['segment_count'])
                count_diff_pct = count_diff / stats[method1]['segment_count'] * 100
                
                # æ€»æ—¶é•¿å·®å¼‚
                duration_diff = abs(stats[method1]['total_speech_duration'] - stats[method2]['total_speech_duration'])
                duration_diff_pct = duration_diff / stats[method1]['total_speech_duration'] * 100
                
                # é‡å ç‡è®¡ç®—
                overlap_rate = calculate_overlap(methods[method1], methods[method2]) * 100
                
                print(f"{method1:12} vs {method2:12} | æ®µæ•°å·®å¼‚: {count_diff:2d}({count_diff_pct:4.1f}%) | "
                      f"æ—¶é•¿å·®å¼‚: {duration_diff:4.1f}s({duration_diff_pct:4.1f}%) | é‡å ç‡: {overlap_rate:5.1f}%")
    
    # 5. æ€§èƒ½è¯„ä¼°
    print(f"\nâš¡ æ€§èƒ½ç‰¹å¾è¯„ä¼°")
    print("-" * 60)
    
    performance_analysis = {
        'Batch': {
            'accuracy': 'â˜…â˜…â˜…â˜…â˜…',
            'latency': 'â˜…â˜†â˜†â˜†â˜†', 
            'memory': 'â˜…â˜…â˜…â˜†â˜†',
            'realtime': 'âŒ',
            'use_case': 'ç¦»çº¿é«˜ç²¾åº¦å¤„ç†'
        },
        'Continuous': {
            'accuracy': 'â˜…â˜…â˜…â˜…â˜†',
            'latency': 'â˜…â˜…â˜…â˜…â˜†',
            'memory': 'â˜…â˜…â˜…â˜…â˜†', 
            'realtime': 'âœ…',
            'use_case': 'å®æ—¶å¤„ç†å¹³è¡¡æ–¹æ¡ˆ'
        },
        'Streaming': {
            'accuracy': 'â˜…â˜…â˜…â˜…â˜…',
            'latency': 'â˜…â˜…â˜…â˜…â˜…',
            'memory': 'â˜…â˜…â˜…â˜…â˜…',
            'realtime': 'âœ…', 
            'use_case': 'å®æ—¶æµå¼å¤„ç†'
        }
    }
    
    print(f"{'æ–¹æ³•':12} | {'ç²¾åº¦':8} | {'å»¶è¿Ÿ':8} | {'å†…å­˜':8} | {'å®æ—¶':6} | é€‚ç”¨åœºæ™¯")
    print("-" * 80)
    for method, perf in performance_analysis.items():
        if method.lower().replace(' ', '') in [m.lower().replace(' ', '') for m in methods.keys()]:
            print(f"{method:12} | {perf['accuracy']:8} | {perf['latency']:8} | "
                  f"{perf['memory']:8} | {perf['realtime']:6} | {perf['use_case']}")
    
    # 6. å…³é”®å‘ç°æ€»ç»“
    print(f"\nğŸ¯ å…³é”®å‘ç°æ€»ç»“")
    print("-" * 60)
    
    findings = []
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ–¹æ³•æ£€æµ‹åˆ°ç›¸åŒæ•°é‡çš„è¯­éŸ³æ®µ
    segment_counts = [stats[method]['segment_count'] for method in stats.keys()]
    if len(set(segment_counts)) == 1:
        findings.append(f"âœ… æ‰€æœ‰æ–¹æ³•æ£€æµ‹åˆ°ç›¸åŒæ•°é‡çš„è¯­éŸ³æ®µ ({segment_counts[0]}æ®µ)")
    else:
        findings.append(f"âš ï¸  ä¸åŒæ–¹æ³•æ£€æµ‹åˆ°çš„è¯­éŸ³æ®µæ•°é‡ä¸ä¸€è‡´: {dict(zip(stats.keys(), segment_counts))}")
    
    # æ£€æŸ¥è¯­éŸ³æ—¶é•¿ä¸€è‡´æ€§
    speech_durations = [stats[method]['total_speech_duration'] for method in stats.keys()]
    duration_variance = max(speech_durations) - min(speech_durations)
    if duration_variance < 1.0:  # å·®å¼‚å°äº1ç§’
        findings.append(f"âœ… è¯­éŸ³æ—¶é•¿æ£€æµ‹é«˜åº¦ä¸€è‡´ (å·®å¼‚ {duration_variance:.1f}s)")
    else:
        findings.append(f"âš ï¸  è¯­éŸ³æ—¶é•¿æ£€æµ‹å­˜åœ¨å·®å¼‚ (å·®å¼‚ {duration_variance:.1f}s)")
    
    # æ£€æŸ¥æµå¼å¤„ç†ä¿®å¤æ•ˆæœ
    if 'Streaming' in stats and 'Batch' in stats:
        if stats['Streaming']['segment_count'] == stats['Batch']['segment_count']:
            findings.append("âœ… æµå¼VADæœ€åä¸€æ®µç¼ºå¤±é—®é¢˜å·²ä¿®å¤")
        else:
            findings.append("âŒ æµå¼VADä»å­˜åœ¨æ®µæ•°ä¸ä¸€è‡´é—®é¢˜")
    
    # æ¨èä½¿ç”¨åœºæ™¯
    findings.append("ğŸ’¡ æ¨èä½¿ç”¨åœºæ™¯:")
    findings.append("   â€¢ ç¦»çº¿é«˜ç²¾åº¦å¤„ç† â†’ Batch VAD")
    findings.append("   â€¢ å®æ—¶å­—å¹•ç”Ÿæˆ â†’ Streaming VAD") 
    findings.append("   â€¢ å¹³è¡¡æ–¹æ¡ˆ â†’ Continuous VAD")
    
    for finding in findings:
        print(f"   {finding}")
    
    print("\n" + "="*80)

    
def analyze_with_fixed_vad_streaming(audio_stream, sample_rate=16000, 
                                    threshold=0.3, min_silence_duration_ms=300, speech_pad_ms=100,
                                    no_audio_input_threshold=0.5):
    """
    åˆ†ææµå¼éŸ³é¢‘æ•°æ®ï¼Œä½¿ç”¨é¢å‘å¯¹è±¡çš„VACProcessor
    
    Args:
        audio_stream: éŸ³é¢‘æµè¿­ä»£å™¨
        sample_rate: é‡‡æ ·ç‡
        threshold: è¯­éŸ³é˜ˆå€¼ (0.0-1.0)
        min_silence_duration_ms: æœ€å°é™éŸ³æŒç»­æ—¶é—´(ms)
        speech_pad_ms: è¯­éŸ³æ®µå¡«å……æ—¶é—´(ms)
        no_audio_input_threshold: æ— éŸ³é¢‘è¾“å…¥é˜ˆå€¼(ç§’)ï¼Œè¶…è¿‡æ­¤æ—¶é—´æ— æ–°æ•°æ®åˆ™å¼ºåˆ¶ç»“æŸ
    """
    # å¯¼å…¥VACå¤„ç†å™¨
    from subtitle_genius.stream.vac_processor import VACProcessor
    
    print(f"\n===== ä½¿ç”¨é¢å‘å¯¹è±¡çš„VACProcessor =====")
    
    # åˆ›å»ºVACå¤„ç†å™¨
    vac_processor = VACProcessor(
        threshold=threshold,
        min_silence_duration_ms=min_silence_duration_ms,
        speech_pad_ms=speech_pad_ms,
        sample_rate=sample_rate,
        processing_chunk_size=512,
        no_audio_input_threshold=no_audio_input_threshold
    )
    
    # å¤„ç†æµå¼éŸ³é¢‘ï¼Œè¿”å›åŸå§‹ç»“æœï¼ˆä¸è½¬æ¢ä¸ºsegmentsï¼‰
    results = vac_processor.process_streaming_audio(audio_stream, return_segments=False, end_stream_flag = True)
    
    return results


def test_streaming_vad(audio_file, chunk_duration=0.128, sample_rate=16000):
    """
    æµ‹è¯•æµå¼VADå¤„ç†
    
    Args:
        audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        chunk_duration: æ¯ä¸ªå—çš„æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        sample_rate: é‡‡æ ·ç‡
        
    Returns:
        list: åŒ…å«è¯­éŸ³æ®µä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæ®µéƒ½æœ‰startã€endå’Œdurationå­—æ®µ
    """
    import time
    
    # åŠ è½½éŸ³é¢‘æ–‡ä»¶
    audio_data, file_sample_rate = sf.read(audio_file)
    
    # ç¡®ä¿éŸ³é¢‘æ˜¯å•å£°é“
    if len(audio_data.shape) > 1:
        audio_data = audio_data[:, 0]
    
    # ç¡®ä¿éŸ³é¢‘æ˜¯float32
    if audio_data.dtype != np.float32:
        audio_data = audio_data.astype(np.float32)
    
    # å¦‚æœéœ€è¦ï¼Œé‡é‡‡æ ·
    if file_sample_rate != sample_rate:
        import librosa
        audio_data = librosa.resample(audio_data, orig_sr=file_sample_rate, target_sr=sample_rate)
    
    # è®¡ç®—æ¯ä¸ªå—çš„æ ·æœ¬æ•°
    # å°†chunk_sizeè®¾ç½®ä¸º512çš„æ•´æ•°å€ï¼Œä¾‹å¦‚chunk_size = 1536ï¼ˆ3*512ï¼‰æˆ–chunk_size = 2048ï¼ˆ4*512ï¼‰
    chunk_size = int(chunk_duration * sample_rate)
    
    # å¼ºåˆ¶æ£€æŸ¥ï¼šç¡®ä¿chunk_sizeæ˜¯512çš„æ•´æ•°å€
    VAD_CHUNK_SIZE = 512  # Silero VADè¦æ±‚çš„å—å¤§å°
    if chunk_size % VAD_CHUNK_SIZE != 0:
        raise ValueError(
            f"chunk_sizeå¿…é¡»æ˜¯{VAD_CHUNK_SIZE}çš„æ•´æ•°å€ï¼å½“å‰å€¼ä¸º{chunk_size}ã€‚"
            f"è¯·è°ƒæ•´chunk_durationä¸º{VAD_CHUNK_SIZE/sample_rate}çš„æ•´æ•°å€ï¼Œ"
            f"ä¾‹å¦‚{VAD_CHUNK_SIZE/sample_rate*3}ç§’æˆ–{VAD_CHUNK_SIZE/sample_rate*4}ç§’ã€‚"
        )
    
    
    # åˆ›å»ºä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¨¡æ‹Ÿæµå¼è¾“å…¥
    def audio_stream_generator():
        for i in range(0, len(audio_data), chunk_size):
            chunk = audio_data[i:min(i+chunk_size, len(audio_data))]
            yield chunk
            # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
            # time.sleep(0.1)
    
    # åˆ›å»ºéŸ³é¢‘æµ
    audio_stream = audio_stream_generator()
    
    # ä½¿ç”¨æµå¼VADå¤„ç†
    streaming_results = analyze_with_fixed_vad_streaming(
        audio_stream,
        sample_rate=sample_rate,
        threshold=0.3,
        min_silence_duration_ms=300,
        speech_pad_ms=100,
        no_audio_input_threshold=0.5
    )
    
    # è½¬æ¢ç»“æœä¸ºè¯­éŸ³æ®µ
    streaming_segments = []
    start_time = None
    
    for result in streaming_results:
        if 'start' in result:
            start_time = result['start']
        elif 'end' in result and start_time is not None:
            streaming_segments.append({
                'start': start_time,
                'end': result['end'],
                'duration': result['end'] - start_time
            })
            start_time = None
    
    return streaming_segments


def test_speech_segment_events(audio_file="chinese_180s.wav", chunk_duration=0.128, sample_rate=16000):
    """
    æµ‹è¯•è¯­éŸ³æ®µäº‹ä»¶è®¢é˜…åŠŸèƒ½
    
    Args:
        audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        chunk_duration: æ¯ä¸ªå—çš„æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        sample_rate: é‡‡æ ·ç‡
    """
    print("\n" + "="*80)
    print("ğŸ¤ æµ‹è¯•è¯­éŸ³æ®µäº‹ä»¶è®¢é˜…åŠŸèƒ½")
    print("="*80)
    
    # äº‹ä»¶æ”¶é›†å™¨
    detected_segments = []
    
    def on_speech_segment_detected(speech_segment):
        """
        è¯­éŸ³æ®µæ£€æµ‹äº‹ä»¶å›è°ƒå‡½æ•°
        
        Args:
            speech_segment: åŒ…å«startã€endã€durationã€audio_bytesã€sample_rateçš„å­—å…¸
        """
        print(f"ğŸ¯ æ£€æµ‹åˆ°è¯­éŸ³æ®µ:")
        print(f"   æ—¶é—´èŒƒå›´: {speech_segment['start']:.2f}s - {speech_segment['end']:.2f}s")
        print(f"   æŒç»­æ—¶é—´: {speech_segment['duration']:.2f}s")
        print(f"   éŸ³é¢‘æ•°æ®: {len(speech_segment['audio_bytes'])} bytes")
        print(f"   é‡‡æ ·ç‡: {speech_segment['sample_rate']} Hz")
        
        # è®¡ç®—éŸ³é¢‘æ•°æ®çš„ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        audio_array = np.frombuffer(speech_segment['audio_bytes'], dtype=np.float32)
        print(f"   éŸ³é¢‘æ ·æœ¬æ•°: {len(audio_array)}")
        print(f"   éŸ³é¢‘RMS: {np.sqrt(np.mean(audio_array**2)):.4f}")
        print(f"   éŸ³é¢‘å³°å€¼: {np.max(np.abs(audio_array)):.4f}")
        
        # éªŒè¯éŸ³é¢‘æ•°æ®é•¿åº¦æ˜¯å¦ä¸æ—¶é•¿åŒ¹é…
        expected_samples = int(speech_segment['duration'] * speech_segment['sample_rate'])
        actual_samples = len(audio_array)
        sample_diff = abs(expected_samples - actual_samples)
        print(f"   æ ·æœ¬æ•°éªŒè¯: æœŸæœ›{expected_samples}, å®é™…{actual_samples}, å·®å¼‚{sample_diff}")
        
        # ä¿å­˜åˆ°æ”¶é›†å™¨
        detected_segments.append({
            'start': speech_segment['start'],
            'end': speech_segment['end'],
            'duration': speech_segment['duration'],
            'audio_bytes_length': len(speech_segment['audio_bytes']),
            'audio_samples': len(audio_array),
            'audio_rms': np.sqrt(np.mean(audio_array**2)),
            'audio_peak': np.max(np.abs(audio_array)),
            'sample_rate': speech_segment['sample_rate']
        })
        
        print(f"   âœ… äº‹ä»¶å¤„ç†å®Œæˆ (æ€»è®¡å·²æ£€æµ‹ {len(detected_segments)} ä¸ªè¯­éŸ³æ®µ)")
        print("-" * 60)
    
    # åŠ è½½éŸ³é¢‘æ–‡ä»¶
    print(f"ğŸ“ åŠ è½½éŸ³é¢‘æ–‡ä»¶: {audio_file}")
    audio_data, file_sample_rate = sf.read(audio_file)
    
    # ç¡®ä¿éŸ³é¢‘æ˜¯å•å£°é“
    if len(audio_data.shape) > 1:
        audio_data = audio_data[:, 0]
        print("ğŸ”„ è½¬æ¢ä¸ºå•å£°é“")
    
    # ç¡®ä¿éŸ³é¢‘æ˜¯float32
    if audio_data.dtype != np.float32:
        audio_data = audio_data.astype(np.float32)
        print("ğŸ”„ è½¬æ¢ä¸ºfloat32æ ¼å¼")
    
    # å¦‚æœéœ€è¦ï¼Œé‡é‡‡æ ·
    if file_sample_rate != sample_rate:
        import librosa
        audio_data = librosa.resample(audio_data, orig_sr=file_sample_rate, target_sr=sample_rate)
        print(f"ğŸ”„ é‡é‡‡æ ·: {file_sample_rate}Hz â†’ {sample_rate}Hz")
    
    print(f"ğŸ“Š éŸ³é¢‘ä¿¡æ¯:")
    print(f"   æ—¶é•¿: {len(audio_data)/sample_rate:.2f}s")
    print(f"   æ ·æœ¬æ•°: {len(audio_data)}")
    print(f"   é‡‡æ ·ç‡: {sample_rate}Hz")
    
    # è®¡ç®—æ¯ä¸ªå—çš„æ ·æœ¬æ•°
    chunk_size = int(chunk_duration * sample_rate)
    
    # ç¡®ä¿chunk_sizeæ˜¯512çš„æ•´æ•°å€
    VAD_CHUNK_SIZE = 512
    if chunk_size % VAD_CHUNK_SIZE != 0:
        # è‡ªåŠ¨è°ƒæ•´åˆ°æœ€è¿‘çš„512çš„æ•´æ•°å€
        chunk_size = ((chunk_size // VAD_CHUNK_SIZE) + 1) * VAD_CHUNK_SIZE
        chunk_duration = chunk_size / sample_rate
        print(f"âš ï¸  è‡ªåŠ¨è°ƒæ•´chunk_sizeåˆ°512çš„æ•´æ•°å€: {chunk_size} (æ—¶é•¿: {chunk_duration:.3f}s)")
    
    print(f"ğŸ”§ å¤„ç†å‚æ•°:")
    print(f"   å—å¤§å°: {chunk_size} æ ·æœ¬ ({chunk_duration:.3f}s)")
    print(f"   VADé˜ˆå€¼: 0.3")
    print(f"   æœ€å°é™éŸ³æ—¶é•¿: 300ms")
    print(f"   è¯­éŸ³å¡«å……: 100ms")
    
    # åˆ›å»ºä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¨¡æ‹Ÿæµå¼è¾“å…¥
    def audio_stream_generator():
        print(f"ğŸš€ å¼€å§‹æµå¼å¤„ç†...")
        for i in range(0, len(audio_data), chunk_size):
            chunk = audio_data[i:min(i+chunk_size, len(audio_data))]
            current_time = i / sample_rate
            print(f"ğŸ“¦ å¤„ç†éŸ³é¢‘å—: {current_time:.2f}s - {(i+len(chunk))/sample_rate:.2f}s ({len(chunk)} æ ·æœ¬)")
            yield chunk
    
    # å¯¼å…¥VACå¤„ç†å™¨
    from subtitle_genius.stream.vac_processor import VACProcessor
    
    # åˆ›å»ºå¸¦äº‹ä»¶å›è°ƒçš„VACå¤„ç†å™¨
    print(f"ğŸ—ï¸  åˆ›å»ºVACProcessor...")
    vac_processor = VACProcessor(
        threshold=0.3,
        min_silence_duration_ms=300,
        speech_pad_ms=100,
        sample_rate=sample_rate,
        processing_chunk_size=512,
        no_audio_input_threshold=0.5,
        on_speech_segment=on_speech_segment_detected  # ğŸ¯ å…³é”®ï¼šè®¾ç½®äº‹ä»¶å›è°ƒ
    )
    
    # åˆ›å»ºéŸ³é¢‘æµ
    audio_stream = audio_stream_generator()
    
    # å¼€å§‹å¤„ç†
    print(f"â–¶ï¸  å¼€å§‹VADå¤„ç†...")
    start_time = time.time()
    
    # å¤„ç†æµå¼éŸ³é¢‘ï¼ˆè¿™ä¼šè§¦å‘äº‹ä»¶å›è°ƒï¼‰
    vad_results = vac_processor.process_streaming_audio(audio_stream, return_segments=False, end_stream_flag=True)
    
    processing_time = time.time() - start_time
    print(f"â±ï¸  å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}s")
    
    # ç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“ˆ å¤„ç†ç»“æœç»Ÿè®¡:")
    print(f"   VADåŸå§‹äº‹ä»¶æ•°: {len(vad_results)}")
    print(f"   æ£€æµ‹åˆ°çš„è¯­éŸ³æ®µæ•°: {len(detected_segments)}")
    
    if detected_segments:
        total_speech_duration = sum(seg['duration'] for seg in detected_segments)
        avg_duration = total_speech_duration / len(detected_segments)
        max_duration = max(seg['duration'] for seg in detected_segments)
        min_duration = min(seg['duration'] for seg in detected_segments)
        
        print(f"   æ€»è¯­éŸ³æ—¶é•¿: {total_speech_duration:.2f}s")
        print(f"   å¹³å‡æ®µé•¿: {avg_duration:.2f}s")
        print(f"   æœ€é•¿æ®µ: {max_duration:.2f}s")
        print(f"   æœ€çŸ­æ®µ: {min_duration:.2f}s")
        print(f"   è¯­éŸ³å æ¯”: {total_speech_duration/(len(audio_data)/sample_rate)*100:.1f}%")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ£€æµ‹åˆ°çš„è¯­éŸ³æ®µ
        print(f"\nğŸ“‹ æ£€æµ‹åˆ°çš„è¯­éŸ³æ®µè¯¦æƒ… (å‰5ä¸ª):")
        print(f"{'#':<3} {'å¼€å§‹(s)':<8} {'ç»“æŸ(s)':<8} {'æ—¶é•¿(s)':<8} {'éŸ³é¢‘(KB)':<10} {'RMS':<8} {'å³°å€¼':<8}")
        print("-" * 70)
        
        for i, seg in enumerate(detected_segments[:5]):
            audio_kb = seg['audio_bytes_length'] / 1024
            print(f"{i+1:<3} {seg['start']:<8.2f} {seg['end']:<8.2f} {seg['duration']:<8.2f} "
                  f"{audio_kb:<10.1f} {seg['audio_rms']:<8.4f} {seg['audio_peak']:<8.4f}")
    
    # éªŒè¯äº‹ä»¶ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ
    print(f"\nâœ… äº‹ä»¶ç³»ç»ŸéªŒè¯:")
    
    # è®¡ç®—é¢„æœŸçš„è¯­éŸ³æ®µæ•°ï¼ˆä»VADåŸå§‹ç»“æœï¼‰
    expected_segments = 0
    in_speech = False
    for result in vad_results:
        if 'start' in result:
            in_speech = True
        elif 'end' in result and in_speech:
            expected_segments += 1
            in_speech = False
    
    print(f"   é¢„æœŸè¯­éŸ³æ®µæ•°: {expected_segments}")
    print(f"   å®é™…è§¦å‘äº‹ä»¶æ•°: {len(detected_segments)}")
    
    if expected_segments == len(detected_segments):
        print(f"   âœ… äº‹ä»¶ç³»ç»Ÿå·¥ä½œæ­£å¸¸ï¼æ‰€æœ‰è¯­éŸ³æ®µéƒ½è§¦å‘äº†äº‹ä»¶")
    else:
        print(f"   âŒ äº‹ä»¶ç³»ç»Ÿå¼‚å¸¸ï¼é¢„æœŸ{expected_segments}ä¸ªäº‹ä»¶ï¼Œå®é™…{len(detected_segments)}ä¸ª")
    
    # æ¨¡æ‹Ÿwhisper_sagemakerè°ƒç”¨
    print(f"\nğŸ¤– æ¨¡æ‹ŸWhisper SageMakerè°ƒç”¨:")
    for i, seg in enumerate(detected_segments[:3]):  # åªæ¨¡æ‹Ÿå‰3ä¸ª
        print(f"   ğŸ“ è°ƒç”¨whisper_sagemaker.transcribe():")
        print(f"      è¯­éŸ³æ®µ {i+1}: {seg['start']:.2f}s-{seg['end']:.2f}s")
        print(f"      éŸ³é¢‘æ•°æ®: {seg['audio_bytes_length']} bytes")
        print(f"      â†’ è¿™é‡Œä¼šè°ƒç”¨å®é™…çš„è½¬å½•æœåŠ¡")
    
    print(f"\nğŸ‰ äº‹ä»¶è®¢é˜…æµ‹è¯•å®Œæˆï¼")
    return detected_segments


if __name__ == "__main__":
    # è¿è¡ŒåŸæœ‰çš„ä¸»æµ‹è¯•
    main()
    
    # # è¿è¡Œæ–°çš„äº‹ä»¶è®¢é˜…æµ‹è¯•
    # print("\n" + "="*100)
    # test_speech_segment_events()

